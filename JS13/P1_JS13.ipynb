{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7162de5d",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raudhil/Machine-Learning-2025/blob/main/Jobsheet%2013/RAUDHIL_FIRDAUS_NAUFAL_JS13_Praktikum3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "982fceb5",
      "metadata": {
        "id": "982fceb5",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "# **Praktikum 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b16d830",
      "metadata": {
        "id": "5b16d830"
      },
      "source": [
        "**Langkah:**\n",
        "\n",
        "Buat dataset sederhana (XOR).\n",
        "\n",
        "Inisialisasi bobot dan bias.\n",
        "\n",
        "Implementasikan forward pass.\n",
        "\n",
        "Hitung error dan lakukan backpropagation.\n",
        "\n",
        "Update bobot menggunakan gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "313ca667",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "313ca667",
        "outputId": "1bc687a6-5aca-4e53-9d12-44b68717ec7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.2540418689873392\n",
            "Epoch 1000, Loss: 0.2489254088154776\n",
            "Epoch 2000, Loss: 0.23535178772453808\n",
            "Epoch 3000, Loss: 0.18195854197618286\n",
            "Epoch 4000, Loss: 0.15147045564123343\n",
            "Epoch 5000, Loss: 0.13993153803281277\n",
            "Epoch 6000, Loss: 0.1348854364974286\n",
            "Epoch 7000, Loss: 0.13221768204053658\n",
            "Epoch 8000, Loss: 0.13060702516584713\n",
            "Epoch 9000, Loss: 0.12954345910055642\n",
            "Prediksi:\n",
            "[[0.04847747]\n",
            " [0.50001619]\n",
            " [0.93245302]\n",
            " [0.50822002]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4562b403",
      "metadata": {
        "id": "4562b403"
      },
      "source": [
        "**Tugas 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45fe538",
      "metadata": {
        "id": "c45fe538"
      },
      "source": [
        "\n",
        "Ubah jumlah neuron hidden layer menjadi 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9444953",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9444953",
        "outputId": "f35be709-f8e7-41d2-b0c7-f30822d384e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.2822343450251595\n",
            "Epoch 1000, Loss: 0.24998175281374235\n",
            "Epoch 2000, Loss: 0.24846388663387\n",
            "Epoch 3000, Loss: 0.2325788958924659\n",
            "Epoch 4000, Loss: 0.10203055464812211\n",
            "Epoch 5000, Loss: 0.021941342678303277\n",
            "Epoch 6000, Loss: 0.009735466937629689\n",
            "Epoch 7000, Loss: 0.0059380428412692645\n",
            "Epoch 8000, Loss: 0.004190087796248304\n",
            "Epoch 9000, Loss: 0.0032069833036875483\n",
            "Prediksi:\n",
            "[[0.03020431]\n",
            " [0.94855612]\n",
            " [0.94878151]\n",
            " [0.06445946]]\n"
          ]
        }
      ],
      "source": [
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6e0f605",
      "metadata": {
        "id": "c6e0f605"
      },
      "source": [
        "Bandingkan hasil loss dengan konfigurasi awal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3dc01c0",
      "metadata": {
        "id": "b3dc01c0"
      },
      "source": [
        "Berdasarkan hasil loss antara konfigurasi awal dengan konfigurasi baru yang menambahkan hidden neuron sebanyak 1, terlihat kalau loss turun hingga 0.0025, walaupun itu kecil, tapi bisa diketahui kalau menambah neuron dapat membantu menurunkan error."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8d63cb",
      "metadata": {
        "id": "2e8d63cb"
      },
      "source": [
        "Tambahkan fungsi aktivasi ReLU dan bandingkan hasil.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f8f8caa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f8f8caa",
        "outputId": "7bfd8cb5-b44d-4877-a198-870bcb04704f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigmoid hidden final loss: 0.0022974157268842183\n",
            "Sigmoid hidden predictions:\n",
            " [[0.025]\n",
            " [0.953]\n",
            " [0.951]\n",
            " [0.063]]\n",
            "ReLU hidden final loss: 0.00035154169220683637\n",
            "ReLU hidden predictions:\n",
            " [[0.03 ]\n",
            " [0.986]\n",
            " [0.986]\n",
            " [0.012]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Hyperparams\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "epochs = 10000\n",
        "np.random.seed(42)\n",
        "\n",
        "# initialize once and copy to ensure fair comparison\n",
        "W1_init = np.random.randn(input_size, hidden_size)\n",
        "b1_init = np.zeros((1, hidden_size))\n",
        "W2_init = np.random.randn(hidden_size, output_size)\n",
        "b2_init = np.zeros((1, output_size))\n",
        "\n",
        "# Aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(a):\n",
        "    return a * (1 - a)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# Train with sigmoid hidden\n",
        "W1 = W1_init.copy(); b1 = b1_init.copy(); W2 = W2_init.copy(); b2 = b2_init.copy()\n",
        "for epoch in range(epochs):\n",
        "    z1 = X.dot(W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = a1.dot(W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    error = y - a2\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = a1.T.dot(d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = d_a2.dot(W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = X.T.dot(d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    W1 += lr * d_W1; b1 += lr * d_b1\n",
        "    W2 += lr * d_W2; b2 += lr * d_b2\n",
        "\n",
        "sigmoid_loss = np.mean(np.square(y - a2))\n",
        "sigmoid_pred = a2.copy()\n",
        "\n",
        "# Train with ReLU hidden (same init)\n",
        "W1 = W1_init.copy(); b1 = b1_init.copy(); W2 = W2_init.copy(); b2 = b2_init.copy()\n",
        "for epoch in range(epochs):\n",
        "    z1 = X.dot(W1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = a1.dot(W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    error = y - a2\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = a1.T.dot(d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = d_a2.dot(W2.T) * relu_derivative(z1)\n",
        "    d_W1 = X.T.dot(d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    W1 += lr * d_W1; b1 += lr * d_b1\n",
        "    W2 += lr * d_W2; b2 += lr * d_b2\n",
        "\n",
        "relu_loss = np.mean(np.square(y - a2))\n",
        "relu_pred = a2.copy()\n",
        "\n",
        "print(\"Sigmoid hidden final loss:\", sigmoid_loss)\n",
        "print(\"Sigmoid hidden predictions:\\n\", np.round(sigmoid_pred, 3))\n",
        "print(\"ReLU hidden final loss:\", relu_loss)\n",
        "print(\"ReLU hidden predictions:\\n\", np.round(relu_pred, 3))\n",
        "# ...existing code..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
